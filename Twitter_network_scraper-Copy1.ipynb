{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give credentials to Twitter developer account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= ''\n",
    "consumer_secret= ''\n",
    "access_token= ''\n",
    "access_token_secret= ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate and create an API object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's start..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scraping</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a function for finding mentions in tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mentions(tweet):\n",
    "    mentions = re.findall(r'@\\w+', tweet)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape 10 latest tweets of Twitter account (specified by a user). The number of tweets can be extended -- for the purposes of ur research we limited only to 10 due to the limitations of the Free Twitter API access. The code below will:\n",
    "1) scrape the tweets, extract username, content and timestamp from these tweets and arrange these data in a dataframe<br>\n",
    "2) extract mentions from each of the tweets posted by the accoun owner<br>\n",
    "3) create 'nodes' and 'edges' lists, where nodes are people mentioned by a user, and edges are connections between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_network = pd.DataFrame()\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "not_iterable = []\n",
    "\n",
    "initial_username = input('Enter screen name of a Twitter user (without @): ')\n",
    "nodes.append('@' + initial_username)\n",
    "not_iterable.append('@' + initial_username)\n",
    "\n",
    "tweets = api.user_timeline(screen_name = f'{initial_username}', count = 10, include_rts = True)\n",
    "\n",
    "for tweet in tweets:\n",
    "    data = [tweet.user.screen_name, tweet.text, tweet.created_at]\n",
    "    df_op = pd.DataFrame(data)\n",
    "    df_network = df_network.append(df_op.T)\n",
    "\n",
    "for index in range(df_network.shape[0]):\n",
    "    mentions_to_list = find_mentions(df_network.iloc[index, 1])\n",
    "    for mention in mentions_to_list:\n",
    "        edges.append([('@'+initial_username), mention])\n",
    "        nodes.append(mention)\n",
    "\n",
    "df_network.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below will go through all of the previously mentioned 'nodes' and do exactly the same -- collect profile names that they mention and append them to 'nodes' and 'edges' list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in tqdm.tqdm(nodes):\n",
    "    if user in not_iterable:\n",
    "        continue\n",
    "    else:\n",
    "        scrap_tweets = api.user_timeline(screen_name = user, count = 10, include_rts = True)\n",
    "\n",
    "        df_network_op = pd.DataFrame()\n",
    "        for tweet in scrap_tweets:\n",
    "            data = [tweet.user.screen_name, tweet.text, tweet.created_at]\n",
    "            df_op = pd.DataFrame(data)\n",
    "            df_network = df_network.append(df_op.T)\n",
    "            df_network_op = df_network_op.append(df_op.T)\n",
    "                \n",
    "        for index in range(df_network_op.shape[0]):\n",
    "            mentions_to_list = find_mentions(df_network_op.iloc[index, 1])\n",
    "            for mention in mentions_to_list:\n",
    "                edges.append([(user), mention])\n",
    "                nodes.append(mention)\n",
    "                \n",
    "        not_iterable.append(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>End of scraping -- time for data processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of unique nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = []\n",
    "\n",
    "for node in nodes:\n",
    "    if node not in unique_nodes:\n",
    "        unique_nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create <strong>df_edges</strong> dataframe (for edges) and <strong>df_unique_nodes</strong> dataframe (for unique nodes) and name the columns correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.DataFrame(edges)\n",
    "df_edges.rename(columns={0:'Source',1:'Target'}, inplace=True)\n",
    "df_edges_copy = df_edges.copy()\n",
    "\n",
    "\n",
    "df_unique_nodes = pd.DataFrame(unique_nodes)\n",
    "df_unique_nodes.reset_index(inplace=True)\n",
    "df_unique_nodes.rename(columns={0:'label', 'index':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the display names from the <strong>df_edges</strong> table with account IDs from the <strong>df_unique_nodes</strong> table. <br><br>According to Gephi rules, edges table should consist of just ID numbers of Source and Target entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges_processed = df_edges.merge(df_unique_nodes, \n",
    "               how='left', \n",
    "               left_on='Source', \n",
    "               right_on='label').drop(['Source', 'label'], axis=1).rename(columns={'id':'Source'}).merge(df_unique_nodes,\n",
    "                                                                                                        how='left',\n",
    "                                                                                                        left_on='Target',\n",
    "                                                                                                        right_on='label').drop(['Target','label'], axis=1).rename(columns={'id':'Target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export both files into CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_nodes.to_csv('nodes.csv')\n",
    "df_edges_processed.to_csv('edges.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
